# ğŸŒ§ï¸ drizzler
Adaptive, host-aware, large-scale HTTP fetcher + YouTube video downloader â€” with intelligent throttling, persistence, retries, and visual observability. <br/>
> âš ï¸ Use responsibly. Only download content you have the rights to.

<p align="center">
    <image src="drizzler.png" width="40%">
</p>

## Why drizzler?
Forget one-file toy scripts that hammer servers and get you blocked. **drizzler ğŸŒ§ï¸** is a battle-tested, production-grade engine built for:
- Fetching millions of URLs without triggering rate limits
- Downloading YouTube videos at scale â€” politely and efficiently
- Surviving restarts, network blips, and server throttling
- Giving you full visibility via logs, histograms, and timelines


## Features
- Dual-Mode Engine
  - Fetch webpages (HTTP)
  - Download videos + metadata + thumbnails (via yt-dlp)
  - Extract captions/subtitles as plain text
  - Generate AI summaries using open-source LLMs
  - All under the same adaptive throttling umbrella.

- YouTube-Optimized by Design
   - Groups *.googlevideo.com CDN hosts to avoid per-host throttling.
   - Automatically strips whitespace/malformed URLs.
   - Applies conservative defaults (start at 0.5â€“1.0 RPS).

- Intelligent Throttling & Concurrency
   - Bounded Token Bucket â€” prevents micro-bursts that trigger 429s.
   - Slow Start Ramp-Up â€” gradually increases load to avoid shocking servers.
   - Per-Host Rate & Concurrency Limits â€” fairness across domains
   - Adaptive Rate Control â€” slows down on 429/503, speeds up on success
   - Circuit Breakers â€” auto-pauses misbehaving hosts to save quota.

- Resilient & Production-Ready
   - Exponential Backoff + Jitter â€” handles transient errors gracefully.
   - Respects Retry-After Headers â€” plays nice with server cooldowns.
   - State Persistence â€” resumes token buckets, cooldowns, and breakers after restart.
   - Graceful Shutdown â€” Ctrl+C saves progress and exits cleanly.
   - Deduplication â€” never fetches the same URL twice.
   - Playlist Support â€” Automatically expands YouTube playlists into individual videos.
   - Visual Progress â€” Real-time terminal progress bars via `rich`.

- Observability & Debugging
   - Structured Logging â€” real-time console + optional file output.
   - ASCII Latency Histogram â€” visualize p50/p90/p95/p99.
   - Per-Worker Timeline (Gantt Chart) â€” debug request patterns visually
   - Detailed Metrics â€” success/error rates, status code distributions.
   - Metrics Callbacks â€” plug in Prometheus, StatsD, or custom exporters.

- Anti-Detection & Politeness
   - Header Rotation â€” randomizes User-Agent, Accept-Language, etc
   - Host-Aware Throttling â€” respects logical service boundaries.- Configurable Concurrency â€” avoid overwhelming targets.

- CLI-First Experience
   - Full command-line interface:
â€ƒâ€ƒ`-write-video`, `--write-info-json`, `--write-thumbnail`, `--simulate`, `--output-dir`, `--concurrency`, `--rate`, `--debug`, `--log-file`
   - Install once, run anywhere: `drizzler "https://youtube.com/..." --write-video`

## Quickstart
```bash
docker run ghcr.io/ziwon/drizzler:latest \
  "https://www.youtube.com/watch?v=SYRlTISvjww" \
  "https://www.youtube.com/watch?v=yebNIHKAC4A" \
  "https://www.youtube.com/watch?v=cPJiPphm8tg" \
  --simulate
```

## Prerequisites
- Python 3.11+
- [uv](https://github.com/astral-sh/uv) installed (`pipx install uv` or see repo)
- [justfile](https://github.com/casey/just) installed (optional)
- [Ollama](https://ollama.ai) installed (optional, for AI summarization)

## Setup
```bash
uv venv
uv pip install -e .
cp .env.sample .env  # edit as needed
```

## Usage
Fetch Webpages (HTTP Mode)
```bash
docker run ghcr.io/ziwon/drizzler:latest \
  "https://httpbin.org/delay/1" \
  "https://httpbin.org/status/200" \
  --rate 2.0 \
  --concurrency 5 \
  --debug
```

Download YouTube Videos
```bash
docker run ghcr.io/ziwon/drizzler:latest \
  "https://www.youtube.com/watch?v=WKY-KFCvm-A" \
  --write-video \
  --write-info-json \
  --write-thumbnail \
  --output-dir ./downloads \
  --concurrency 3 \
  --rate 0.8 \
  --debug
```

Download Videos with Captions/Subtitles
```bash
docker run ghcr.io/ziwon/drizzler:latest \
  "https://www.youtube.com/watch?v=Ljf671BSu2g" \
  --write-video \
  --write-subs \
  --output-dir ./downloads \
  --rate 0.5
```

Extract Text Only from Captions (removes timestamps)
```bash
docker run ghcr.io/ziwon/drizzler:latest \
  "https://www.youtube.com/watch?v=JvvQTFqWv-U" \
  --write-txt \
  --output-dir ./downloads
```

Download Both Subtitles and Extracted Text
```bash
docker run ghcr.io/ziwon/drizzler:latest \
  "https://www.youtube.com/watch?v=Ljf671BSu2g" \
  --write-subs \
  --write-txt \
  --output-dir ./downloads
```

Download YouTube Playlists
```bash
# Simply provide a playlist URL, and drizzler will expand it.
# Support for both "watch with list" and "playlist?list=" formats.
docker run ghcr.io/ziwon/drizzler:latest \
  "https://www.youtube.com/playlist?list=PLoROMvodv4rMC33Ucp4aumGNn8SpjEork" \
  --write-info-json \
  --output-dir ./downloads
```

Generate AI Summary with Ollama (Requires Ollama Running)
```
# First, install and run Ollama with a model
ollama pull qwen2.5:3b  # Multilingual model (Chinese/English/Korean)
ollama serve  # Run in another terminal

# Then use drizzler with summarization
docker run ghcr.io/ziwon/drizzler:latest \
  "https://www.youtube.com/watch?v=JvvQTFqWv-U" \
  --summarize \
  --llm-model qwen2.5:3b \
  --output-dir ./downloads

cat ./downloads/JvvQTFqWv-U.ko.summary.md
---
video_id: JvvQTFqWv-U
language: ko
model: qwen2.5:3b
provider: ollama
---

# ë™ì˜ìƒ ë‚´ìš© ìš”ì•½

## ì£¼ìš” ë‚´ìš©
- ë‡Œì§ˆí™˜ í™˜ì ì¤‘ ì¼ë¶€ëŠ” ìš´ë™ìœ¼ë¡œ ë³‘ì˜ ì§„í–‰ì„ ëŠ¦ì¶œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- 5ë¶„ì—ì„œ 10ë¶„ ì •ë„ì˜ ë‹¬ë¦¬ê¸°ë§Œ í•˜ë©´ ì‚¬ë§ë¥ ì´ ë°˜ìœ¼ë¡œ ì¤„ì–´ë“œëŠ” ì—°êµ¬ ê²°ê³¼ê°€ ìˆë‹¤.
- íŒŒí‚¨ìŠ¨ë³‘ ê°™ì€ ì§ˆí™˜ì—ì„œëŠ” ì•½ë³´ë‹¤ ìš´ë™ì´ ë” ë§ì€ íš¨ê³¼ë¥¼ ë³´ì¸ë‹¤.
- ê³ í˜ˆì••ì´ë‚˜ ë‹¹ë‡¨ ë“± ë‹¤ë¥¸ ì§ˆí™˜ì—ì„œë„ ìš´ë™ì€ ì•½ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤.
- ìš°ìš¸ì¦ í™˜ìëŠ” ë‹¬ë¦¬ê¸°ë¥¼ ì‹œì‘í•˜ë©´ ì•½ì„ ì¤„ì´ê³ , ì¼ë¶€ëŠ” ì•½ë„ ëŠì—ˆë‹¤.

## ìƒì„¸ ìš”ì•½
ì›ë¬¸ì—ì„œ ì–¸ê¸‰ëœ ì£¼ìš” ë‚´ìš©ë“¤ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. ë‡Œì§ˆí™˜ í™˜ìë“¤ ì¤‘ ì¼ë¶€ê°€ ìš´ë™ìœ¼ë¡œ ë³‘ì˜ ì§„í–‰ì„ ëŠ¦ì¹  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ ì—°êµ¬ ê²°ê³¼ë¡œ ì…ì¦ë˜ì—ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 5ë¶„ì—ì„œ 10ë¶„ ì •ë„ë§Œ ë‹¬ë¦¬ë©´ ì‚¬ë§ë¥ ì´ ë°˜ìœ¼ë¡œ ì¤„ì–´ë“œëŠ” ì—°êµ¬ ê²°ê³¼ë‹¤. ë˜í•œ íŒŒí‚¨ìŠ¨ë³‘ ê°™ì€ ì§ˆí™˜ì—ì„œëŠ” ì•½ë³´ë‹¤ ìš´ë™ì´ ë” ë§ì€ íš¨ê³¼ë¥¼ ë³´ì˜€ìœ¼ë©°, ê³ í˜ˆì••ì´ë‚˜ ë‹¹ë‡¨ í™˜ìë“¤ë„ ì´ì— í•´ë‹¹í•œë‹¤. ìš°ìš¸ì¦ í™˜ìëŠ” ì•½ì„ ëŠê¸° ì „ ë‹¬ë¦¬ê¸°ë¥¼ ì‹œì‘í•´ ì•½ì„ ì¤„ì´ê³ , ì¼ë¶€ëŠ” ì•½ë„ ë–¨ì–´ì§„ ì‚¬ë¡€ê°€ ìˆë‹¤.
```

Alternative Models for Summarization
```
# For general multilingual
ollama pull gemma2:2b
ollama pull llama3.2:3b

# Use with drizzler
drizzler <url> --summarize --llm-model gemma2:2b
```

Simulate Only (Metadata Extraction, No File Writes)
```bash
docker run ghcr.io/ziwon/drizzler:latest \
  "https://www.youtube.com/watch?v=m0Db3CAxzsE" \
  --simulate \
  --debug
```

Log to File + Quiet Run
```bash
docker run ghcr.io/ziwon/drizzler:latest \
  "https://www.youtube.com/watch?v=fAgAE9JmnOs" \
  --write-video \
  --log-file drizzler_run.log
```

## Sample Output
```
$ drizzler \
  "https://www.youtube.com/watch?v=m0Db3CAxzsE" \
  "https://www.youtube.com/watch?v=fAgAE9JmnOs" \
  "https://www.youtube.com/watch?v=WKY-KFCvm-A" \
  --write-video \
  --write-info-json \
  --write-thumbnail \
  --output-dir ./downloads \
  --concurrency 3 \
  --rate 0.8
2025-09-06 20:16:20 | INFO     | drizzler.core        | Initialized Drizzler with 3 URLs, global_concurrency=3, per_host_rate=0.8
2025-09-06 20:16:20 | INFO     | root                 | Starting Drizzler with 3 URLs | Mode: DOWNLOAD | Concurrency: 3 | Rate: 0.8 RPS
2025-09-06 20:16:20 | INFO     | drizzler.core        | Starting Drizzler run...
2025-09-06 20:16:20 | INFO     | drizzler.persistence | Loaded state for 2 buckets and 2 breakers
2025-09-06 20:16:20 | INFO     | drizzler.core        | Loaded 2 persisted buckets, 2 breakers
2025-09-06 20:16:20 | INFO     | drizzler.throttling  | Token bucket 'httpbin.org' started
2025-09-06 20:16:20 | INFO     | drizzler.throttling  | Token bucket 'youtube-frontend' started
2025-09-06 20:16:20 | INFO     | drizzler.core        | Starting 3 requests with 3 workers
2025-09-06 20:16:24 | INFO     | drizzler.core        | [W2] Downloaded https://www.youtube.com/watch?v=WKY-KFCvm-A in 4.16s
2025-09-06 20:16:25 | INFO     | drizzler.core        | [W1] Downloaded https://www.youtube.com/watch?v=fAgAE9JmnOs in 5.38s
2025-09-06 20:16:29 | INFO     | drizzler.core        | [W0] Downloaded https://www.youtube.com/watch?v=m0Db3CAxzsE in 8.72s
2025-09-06 20:16:29 | INFO     | drizzler.persistence | State saved to drizzler_state.json
2025-09-06 20:16:29 | INFO     | drizzler.core        | State persisted
2025-09-06 20:16:29 | INFO     | drizzler.throttling  | Token bucket 'httpbin.org' stopped
2025-09-06 20:16:32 | INFO     | drizzler.throttling  | Token bucket 'youtube-frontend' stopped
2025-09-06 20:16:32 | INFO     | drizzler.metrics     | Stats computed: success=3, errors=0, mean=6.086s, p95=5.380s, error_rate=0.0%

============================================================
Latency Histogram
4.160s â€“ 4.388s | ######################################## (1)
4.388s â€“ 4.616s | # (0)
4.616s â€“ 4.844s | # (0)
4.844s â€“ 5.072s | # (0)
5.072s â€“ 5.299s | # (0)
5.299s â€“ 5.527s | ######################################## (1)
5.527s â€“ 5.755s | # (0)
5.755s â€“ 5.983s | # (0)
5.983s â€“ 6.211s | # (0)
6.211s â€“ 6.439s | # (0)
6.439s â€“ 6.667s | # (0)
6.667s â€“ 6.894s | # (0)
6.894s â€“ 7.122s | # (0)
7.122s â€“ 7.350s | # (0)
7.350s â€“ 7.578s | # (0)
7.578s â€“ 7.806s | # (0)
7.806s â€“ 8.034s | # (0)
8.034s â€“ 8.262s | # (0)
8.262s â€“ 8.490s | # (0)
8.490s â€“ 8.717s | ######################################## (1)

Request Timeline (relative seconds)
W00 |================================================================================|
W01 |=================================================                               |
W02 |======================================                                          |
0s                                                                          ~ 8.72s
============================================================
2025-09-06 20:16:32 | INFO     | drizzler.core        | Run completed: 3 successes, 0 errors, error_rate=0.00%
2025-09-06 20:16:32 | INFO     | root                 | Run completed: 3 succeeded, 0 failed | Error rate: 0.0% | Mean latency: 6.09s
```

## CLI Options
```
$ drizzler --help
usage: drizzler [-h] [--write-video] [--write-info-json] [--write-thumbnail] [--write-subs] [--write-txt] [--summarize] [--llm-provider {ollama,transformers}] [--llm-model MODEL] [--simulate] [-o OUTPUT_DIR] [--concurrency CONCURRENCY] [--rate RATE] [--debug] [--log-file LOG_FILE] urls [urls ...]

ğŸŒ§ï¸ Drizzler: Adaptive HTTP/Video Downloader with Host-Aware Throttling

positional arguments:
  urls                  YouTube URLs or HTTP endpoints to fetch/download

options:
  -h, --help            show this help message and exit
  --write-video         Download actual video file (via yt-dlp) (default: False)
  --write-info-json     Write video metadata to .info.json file (default: False)
  --write-thumbnail     Download thumbnail image (default: False)
  --write-subs         Download subtitles/captions (default: False)
  --write-txt          Extract text only from captions (removes timestamps and metadata) (default: False)
  --summarize          Generate AI summary of caption text in markdown format (default: False)
  --llm-provider       LLM provider for summarization: ollama or transformers (default: ollama)
  --llm-model          Model to use for summarization (default: qwen2.5:3b)
  --simulate            Simulate only â€” fetch webpage or metadata, but do NOT download files (default: False)
  -o OUTPUT_DIR, --output-dir OUTPUT_DIR
                        Output directory for downloads (default: ./downloads)
  --concurrency CONCURRENCY
                        Global concurrency (reduce for video downloads) (default: 5)
  --rate RATE           Per-host rate limit (requests per second) (default: 1.0)
  --debug               Enable debug-level logging (default: False)
  --no-progress        Disable terminal progress bar (default: False)
```

## Downloading 1M URLs
 Let's deploy drizzler on a 3-node Kubernetes cluster, each with 256 CPU cores and 512GB RAM, is a high-scale, enterprise-grade deployment. Weâ€™re likely downloading millions of videos or scraping at massive scale.

### Goal
Run drizzler as a Kubernetes Job or Deployment (depending on use case) with optimal:

- Pod resource requests/limits
- Concurrency and rate settings
- Anti-throttling safeguards
- Logging, monitoring, and restart policies

### Assumptions
- Downloading YouTube videos â†’ high I/O, moderate CPU, network-bound.
- Each node: 256 CPU / 512GB RAM â†’ very powerful â†’ we can run many pods per node.
- To maximize parallelism without triggering YouTube rate limits or CDN blocks.
- `--write-video` â†’ heavy on disk I/O and network.

### Architecture overview
```
[3 Kubernetes Nodes]
â”‚
â”œâ”€â”€ [Pod 1] drizzler --concurrency 10 --rate 0.5 --urls [batch-1]
â”œâ”€â”€ [Pod 2] drizzler --concurrency 8  --rate 0.5 --urls [batch-2]
â”œâ”€â”€ [Pod 3] drizzler --concurrency 10 --rate 0.5 --urls [batch-3]
...
â””â”€â”€ [Pod N] ...
```

#### STEP 1: Containerize drizzler
```Dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system deps for yt-dlp (including ffmpeg for video processing)
RUN apt-get update && apt-get install -y \
    git \
    ca-certificates \
    brotli \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# Install uv
RUN pip install --no-cache-dir uv

COPY pyproject.toml uv.lock README.md ./
COPY src/ ./src/

RUN uv pip install --system --no-cache -e .

# Create downloads dir (In prod, it mounts to the NAS storage)
RUN mkdir -p /downloads

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# Default command
ENTRYPOINT ["python", "-m", "drizzler.cli"]
```

#### STEP 2: OPTIMIZE DRIZZLER CONFIG FOR K8S
```bash
drizzler \
  --urls "URL1" "URL2" ... "URL50" \
  --write-video \
  --write-info-json \
  --write-thumbnail \
  --output-dir /downloads \
  --concurrency 8 \          # â† Conservative for video downloads
  --rate 0.5 \               # â† Per host (youtube-frontend, youtube-cdn)
  --log-file /logs/drizzler.log
```
  - 8 concurrency per pod â†’ balances CPU, network, and disk I/O.
  - 0.5 RPS per host â†’ avoids triggering YouTubeâ€™s frontend or CDN limits.
  - 50 URLs per pod â†’ manageable batch size.

#### STEP 3: KUBERNETES DEPLOYMENT â€” JOB PER BATCH (RECOMMENDED)

```yaml
# job-template.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: drizzler-job-{{batch_id}}
  labels:
    app: drizzler
spec:
  parallelism: 1
  completions: 1
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: drizzler
    spec:
      restartPolicy: OnFailure
      containers:
        - name: drizzler
          image: your-registry/drizzler:latest
          command: ["python", "-m", "drizzler.cli"]
          args:
            - "--write-video"
            - "--write-info-json"
            - "--write-thumbnail"
            - "--output-dir"
            - "/downloads"
            - "--concurrency"
            - "8"
            - "--rate"
            - "0.5"
            - "--log-file"
            - "/logs/drizzler.log"
            - "https://youtube.com/watch?v=..."
            - "https://youtube.com/watch?v=..."
            # ... up to 50 URLs
          resources:
            requests:
              cpu: "4"
              memory: "8Gi"
            limits:
              cpu: "8"
              memory: "16Gi"
          volumeMounts:
            - name: downloads
              mountPath: /downloads
            - name: logs
              mountPath: /logs
      volumes:
        - name: downloads
          persistentVolumeClaim:
            claimName: drizzler-downloads-pvc
        - name: logs
          persistentVolumeClaim:
            claimName: drizzler-logs-pvc
```
####  STEP 4: RESOURCE ALLOCATION â€” HOW MANY PODS PER NODE?
Each pod:
- Requests: 4 CPU, 8Gi RAM
- Limits: 8 CPU, 16Gi RAM

Each node: 256 CPU, 512Gi RAM
- Max pods per node (by CPU request): 256 / 4 = 64 pods
- Max pods per node (by RAM request): 512 / 8 = 64 pods

Perfectly balanced â†’ 64 pods per node â†’ 192 pods total

> ğŸ’¡You can go higher (e.g., 128 pods/node) by reducing requests to 2 CPU / 4Gi â€” but 64 is safer for I/O-heavy work.

#### STEP 5: PVC SETUP â€” PERSISTENT STORAGE

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: drizzler-downloads-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Ti
  storageClassName: fast-ssd  # â† Use your storage class

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: drizzler-logs-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 100Gi
  storageClassName: fast-ssd
```
  Use `ReadWriteMany` if multiple pods write to same volume (e.g., NFS, CephFS, EFS, GPFS).

> ğŸ’¡ If not available, mount node-local storage or use separate PVCs per pod.

#### STEP 6: DEPLOYMENT STRATEGY â€” BATCH PROCESSING
Use a Job Controller (e.g., Argo Workflows, Kueue, or simple script) to:
- Split 1M URLs into 20,000 batches of 50 URLs.
- Submit 192 Jobs (max parallel) â†’ as Jobs complete, submit more.
- Monitor success/failure â†’ retry failed batches.

```bash
#!/bin/bash
# launch-jobs.sh
BATCH_SIZE=50
MAX_JOBS=192

URLS=($(cat urls.txt))  # One URL per line
TOTAL=${#URLS[@]}

for ((i=0; i<TOTAL; i+=BATCH_SIZE)); do
    while [[ $(kubectl get jobs -l app=drizzler --no-headers | wc -l) -ge $MAX_JOBS ]]; do
        sleep 10
    done

    BATCH=("${URLS[@]:i:BATCH_SIZE}")
    JOB_NAME="drizzler-job-$(date +%s)-$i"

    # Generate job YAML with these URLs
    envsubst <<EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: $JOB_NAME
  labels:
    app: drizzler
spec:
  template:
    spec:
      containers:
      - name: drizzler
        image: your-registry/drizzler:latest
        command: ["python", "-m", "drizzler.cli"]
        args:
          - "--write-video"
          - "--write-info-json"
          - "--write-thumbnail"
          - "--output-dir"
          - "/downloads"
          - "--concurrency"
          - "8"
          - "--rate"
          - "0.5"
          - "--log-file"
          - "/logs/\$JOB_NAME.log"
$(for url in "${BATCH[@]}"; do echo "          - \"$url\""; done)
        resources:
          requests:
            cpu: "4"
            memory: "8Gi"
          limits:
            cpu: "8"
            memory: "16Gi"
        volumeMounts:
          - name: downloads
            mountPath: /downloads
          - name: logs
            mountPath: /logs
      restartPolicy: OnFailure
      volumes:
        - name: downloads
          persistentVolumeClaim:
            claimName: drizzler-downloads-pvc
        - name: logs
          persistentVolumeClaim:
            claimName: drizzler-logs-pvc
EOF
done
```

####  STEP 7: MONITORING & OBSERVABILITY
In core.py, expose metrics via callback:

```python
from prometheus_client import Counter, Histogram, start_http_server

REQUESTS_TOTAL = Counter('drizzler_requests_total', 'Total requests', ['status'])
LATENCY_HIST = Histogram('drizzler_request_latency_seconds', 'Request latency')

def prometheus_callback(stats_dict):
    for status, count in stats_dict.get("status_counts", {}).items():
        REQUESTS_TOTAL.labels(status=status).inc(count)
    for lat in self.latencies:  # need to store or pass
        LATENCY_HIST.observe(lat)
```
Then expose /metrics endpoint.

#### STEP 8: ANTI-THROTTLING BEST PRACTICES
Even with 192 pods, you must avoid triggering YouTubeâ€™s global/IP-based rate limits.

**Use Proxy Rotation (CRITICAL)**
Modify `core.py` to rotate proxies
```python
# In _fetch_once
proxies = [
    "http://proxy1:3128",
    "http://proxy2:3128",
    # ... or use rotating proxy service
]
proxy = random.choice(proxies)

async with session.get(url, headers=headers, proxy=proxy) as resp:
```
> ğŸ’¡ Use residential proxies (e.g., BrightData, Oxylabs) â€” datacenter IPs get blocked fast.


â–¶ Limit Total RPS per IP

With 192 pods Ã— 0.5 RPS = 96 RPS per IP â†’ **TOO HIGH** for YouTube.

*SOLUTION*: Use proxy per pod or limit global concurrency.


#### FINAL RECOMMENDED SETTINGS PER POD
| Setting             | Value   | Why                          |
| ------------------- | ------- | ---------------------------- |
| `--concurrency`     | 6â€“8     | Balance CPU, network, disk   |
| `--rate`            | 0.3â€“0.5 | Conservative for YouTube     |
| CPU request         | 2       | Leave room for system        |
| Memory request      | 8 Gi    | yt-dlp can be memory-heavy   |
| URLs per pod        | 30â€“50   | Small batches = easy retries |
| Max pods per node   | 64      | 256 CPU / 4 = 64             |
| Total parallel pods | 192     | 3 nodes Ã— 64                 |

### SCALING BEYOND â€” IF NEEDED
- Add more nodes â†’ linear scale.
- Use Kueue or Argo Workflows for advanced job scheduling.
- Add horizontal pod autoscaler if using Deployment (not Job).
- Use Redis to coordinate global rate limits across pods.

## Contribute
PRs welcome! Ideas:
- Add progress bars
- Support playlists/channels
- Export to Prometheus
- Dockerize for cluster deployment

---

Happy drizzling! ğŸŒ§ï¸ğŸ¥ğŸ“Š
